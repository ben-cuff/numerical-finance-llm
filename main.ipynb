{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below generates all the necessary imports and creates the client for using the openai or deepseek models. You can easily switch between models by changing the base_url in the client constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The system cannot find the path specified.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotenv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall python-dotenv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdotenv\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mimportlib\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmessages\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'dotenv'"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import dotenv\n",
    "import importlib\n",
    "import messages\n",
    "\n",
    "\n",
    "importlib.reload(messages)\n",
    "from messages import messages_list\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "executor = ThreadPoolExecutor()\n",
    "\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "openai_api_key = dotenv.get_key(\".env\", \"OPENAI_API_KEY\")\n",
    "deepseek_api_key = dotenv.get_key(\".env\", \"DEEPSEEK_API_KEY\")\n",
    "\n",
    "responses = []\n",
    "\n",
    "deepseek_url = \"https://api.deepseek.com\"\n",
    "openai_url = \"https://api.openai.com/v1\"\n",
    "\n",
    "client = OpenAI(api_key=openai_api_key,  base_url=openai_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates an array of messages containing general instructions for the LLM as well as the question that it should answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\n",
    "        [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"You are trying to help people that are not very knowledgeable about finance answer questions about their mortgage.\",\n",
    "            },\n",
    "            {\"role\": msg[\"role\"], \"content\": msg[\"content\"]},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Output only the answer you get.\",\n",
    "            },\n",
    "        ],\n",
    "        i,\n",
    "    )\n",
    "    for i, msg in enumerate(messages_list)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "helper function that cleans the response to make it only numbers so we can easily compare with the answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def extract_number(s):\n",
    "    # Remove backticks and extract the numeric part\n",
    "    s = s.replace(\"`\", \"\")\n",
    "    match = re.search(r\"(-?\\$?[\\d,]+(?:\\.\\d+)?)\", s)\n",
    "    if match:\n",
    "        return float(match.group(1).replace(\",\", \"\").replace(\"$\", \"\"))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates an array of ai_models to make it simpler to swap between them. In order to use deepseek-chat, make sure you change the base_url for the client up top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_models = [\n",
    "    \"deepseek-chat\", # 0\n",
    "    \"gpt-3.5-turbo\", # 1\n",
    "    \"gpt-4\",         # 2\n",
    "    \"o1-mini\",       # 3\n",
    "    \"gpt-4o\",        # 4\n",
    "    \"gpt-4o-mini\",   # 5\n",
    "    \"o3-mini\",       # 6\n",
    "    \"o1\",            # 7\n",
    "    \"o1-preview\",    # 8\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gets all of the responses using a particular AI model from above. It does this in parallel to improve speed using an executor. \n",
    "\n",
    "Temperature - a variable used to increase or decrease the variabilty of a response. It ranges from 0 - 2 with 2 being the most random, 0 being the least.\n",
    "\n",
    "Max Completion tokens - sets an upper bound on tokens that can be used on a completion. In general, each token is around 3/4 of a word.\n",
    "\n",
    "Reasoning Effort - can be either low, medium, or high and it specifies how hard a reasoning model will try. If set to low, responses will be faster and use less tokens, but they will also use less reasoning. Only on o1 and o3-mini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 1\n",
    "max_completion_tokens = 2000\n",
    "reasoning_effort = [\"low\", \"medium\", \"high\"]\n",
    "\n",
    "\n",
    "def get_response(message):\n",
    "    response = client.chat.completions.create(\n",
    "        model=ai_models[6],\n",
    "        temperature=temperature,\n",
    "        max_completion_tokens=max_completion_tokens,\n",
    "        messages=message[0],\n",
    "        # reasoning_effort=reasoning_effort[1]\n",
    "    )\n",
    "    return (response.choices[0].message.content, message[1])\n",
    "\n",
    "\n",
    "futures = [executor.submit(get_response, message) for message in messages]\n",
    "responses_parallel = [future.result() for future in futures]\n",
    "responses_parallel = sorted(responses_parallel, key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cleans the responses from above and outputs them compared to the expected answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0:\n",
      "Response: 336.38\n",
      "Expected: 336.37\n",
      "\n",
      "Question 1:\n",
      "Response: 19.77\n",
      "Expected: 19.77\n",
      "\n",
      "Question 2:\n",
      "Response: 88.81\n",
      "Expected: 88.85\n",
      "\n",
      "Question 3:\n",
      "Response: 1638.62\n",
      "Expected: 1637.97\n",
      "\n",
      "Question 4:\n",
      "Response: 1713.36\n",
      "Expected: 1713.37\n",
      "\n",
      "Question 5:\n",
      "Response: 44289.0\n",
      "Expected: 44289.03\n",
      "\n",
      "Question 6:\n",
      "Response: 171867.0\n",
      "Expected: 171836\n",
      "\n",
      "Question 7:\n",
      "Response: -1957.06\n",
      "Expected: -1954.91\n",
      "\n",
      "Question 8:\n",
      "Response: 15.0\n",
      "Expected: 15\n",
      "\n",
      "Question 9:\n",
      "Response: 909.09\n",
      "Expected: 909.09\n",
      "\n",
      "Question 10:\n",
      "Response: 64843.56\n",
      "Expected: 64843.56\n",
      "\n",
      "Question 11:\n",
      "Response: 15.9374246\n",
      "Expected: 15.94\n",
      "\n",
      "Question 12:\n",
      "Response: 1869.16\n",
      "Expected: 1869.07\n",
      "\n",
      "Question 13:\n",
      "Response: 11960.0\n",
      "Expected: 11969.6\n",
      "\n",
      "Question 14:\n",
      "Response: 388000.0\n",
      "Expected: 388075.82\n",
      "\n",
      "Question 15:\n",
      "Response: 8.51\n",
      "Expected: 8.51\n",
      "\n",
      "Question 16:\n",
      "Response: 570457.0\n",
      "Expected: 570455.96\n",
      "\n",
      "Question 17:\n",
      "Response: 75.13\n",
      "Expected: 74.7\n",
      "\n",
      "Question 18:\n",
      "Response: 123542.0\n",
      "Expected: 123592.35\n",
      "\n",
      "Question 19:\n",
      "Response: 5148.19\n",
      "Expected: 5122.28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleaned_responses = [(extract_number(text)) for text,_ in responses_parallel]\n",
    "\n",
    "for i, (response, message) in enumerate(zip(cleaned_responses, messages_list)):\n",
    "    print(f\"Question {i}:\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"Expected: {message['answer']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates two assistants, one that has openai's code interpreter and another that does not. The instructions are similar for the two but the first one tells the assistant to use python scripts to answer the problem. Both models are told to only output the answer they get to make data collection easier. The assistants feature is only available on the gpt-4o and gpt-4o-mini models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_instructions = \"You are trying to help people that are not very knowledgeable about finance answer questions about their mortgage. Use python scripts to solve for things like the time value of money. Output only the answer you get after running the python script. Please make sure the only output you have is a number\"\n",
    "assistant_name = \"Finance Advisor With Python\"\n",
    "\n",
    "my_assistant = client.beta.assistants.create(\n",
    "    instructions=assistant_instructions,\n",
    "    name=assistant_name,\n",
    "    tools=[{\"type\": \"code_interpreter\"}],\n",
    "    model=ai_models[5],\n",
    ")\n",
    "\n",
    "assistant_instructions_without_python = \"You are trying to help people that are not very knowledgeable about finance answer questions about their mortgage. Output only the answer you get. Please make sure the only output you have is a number\"\n",
    "assistant_name_without_python = \"Finance Advisor No Python\"\n",
    "\n",
    "my_assistant_without_python = client.beta.assistants.create(\n",
    "    instructions=assistant_instructions_without_python,\n",
    "    name=assistant_name_without_python,\n",
    "    model=ai_models[5],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below is a function that handles a message to an assistant. It leverages threads from openai to do this and then returns the answer text and index when it is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_message(idx, msg, assistant):\n",
    "    thread = client.beta.threads.create()\n",
    "\n",
    "    client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=msg[\"content\"],\n",
    "    )\n",
    "\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id=assistant.id,\n",
    "    )\n",
    "\n",
    "    if run.status == \"completed\":\n",
    "        messages_assistant = client.beta.threads.messages.list(thread_id=thread.id)\n",
    "        answer_text = messages_assistant.data[0].content[0].text.value\n",
    "        return (answer_text, idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates responses for the assistant that can use and run python and then cleans the responses so it is just the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = [\n",
    "    executor.submit(process_message, idx, msg, my_assistant)\n",
    "    for idx, msg in enumerate(messages_list)\n",
    "]\n",
    "responses_assistant = [future.result() for future in futures]\n",
    "\n",
    "cleaned_responses_python = [\n",
    "    round(extract_number(text), 2) for text, idx in responses_assistant\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generates the responses for the model without python using the same logic as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "futures = [\n",
    "    executor.submit(process_message, idx, msg, my_assistant_without_python)\n",
    "    for idx, msg in enumerate(messages_list)\n",
    "]\n",
    "responses_assistant_without_python = [future.result() for future in futures]\n",
    "\n",
    "cleaned_responses_without_python = [\n",
    "    round(extract_number(text), 2) for text, idx in responses_assistant_without_python\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responses no python     ['      335.48', '       19.16', '       76.66', '    1,500.94', '    1,391.23', '   44,676.34', '  155,654.00', '      701.28', '       12.00', '      909.09', '   64,150.14', '       15.93', '    1,360.49', '   20,117.21', '  386,195.34', '        9.64', '  749,688.93', '      102.77', '  196,242.21', '      727.29']\n",
      "Responses using python  ['      336.37', '       19.77', '       88.85', '    1,637.97', '    1,713.37', '   44,289.03', '  171,835.94', '   -1,954.91', '       15.00', '      909.09', '   64,843.56', '       15.94', '    1,869.07', '   11,969.60', '  388,075.83', '        8.51', '  570,455.96', '       74.70', '  123,592.35', '    5,122.28']\n",
      "Answers                 ['      336.37', '       19.77', '       88.85', '    1,637.97', '    1,713.37', '   44,289.03', '  171,836.00', '   -1,954.91', '       15.00', '      909.09', '   64,843.56', '       15.94', '    1,869.07', '   11,969.60', '  388,075.82', '        8.51', '  570,455.96', '       74.70', '  123,592.35', '    5,122.28']\n"
     ]
    }
   ],
   "source": [
    "print(\"Responses no python    \", [f\"{x:12,.2f}\" for x in cleaned_responses_without_python])\n",
    "print(\"Responses using python \", [f\"{x:12,.2f}\" for x in cleaned_responses_python])\n",
    "print(\"Answers                \", [f\"{x['answer']:12,.2f}\" for x in messages_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This calculates the percent error for with and without python by comparing it to the correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean error without python: 26.71%\n",
      "mean error with python: 0.0%\n"
     ]
    }
   ],
   "source": [
    "def percent_error(pred, actual):\n",
    "    if actual != 0:\n",
    "        return abs(pred - actual) / abs(actual) * 100\n",
    "    return float(\"nan\")\n",
    "\n",
    "\n",
    "percent_errors_python = [\n",
    "    percent_error(pred, msg['answer'])\n",
    "    for pred, msg in zip(cleaned_responses_python, messages_list)\n",
    "]\n",
    "percent_errors_without_python = [\n",
    "    percent_error(pred, msg['answer'])\n",
    "    for pred, msg in zip(cleaned_responses_without_python, messages_list)\n",
    "]\n",
    "\n",
    "print(\n",
    "    f\"mean error without python: {round(sum(percent_errors_without_python) / len(percent_errors_without_python), 2)}%\"\n",
    ")\n",
    "print(\n",
    "    f\"mean error with python: {round(sum(percent_errors_python) / len(percent_errors_python), 2)}%\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
